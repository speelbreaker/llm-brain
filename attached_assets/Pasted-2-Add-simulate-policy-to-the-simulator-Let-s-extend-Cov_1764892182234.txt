2. Add simulate_policy(...) to the simulator

Let’s extend CoveredCallSimulator with a simple simulate_policy and then a function that emits (state, action, reward) rows for training.

2.1 Updated covered_call_simulator.py

Here’s a version that adds:

_generate_decision_times

simulate_policy(...) that loops over decision times, calls simulate_single_call, and builds a rough equity curve (assuming trades don’t massively overlap).

A very simple policy_fn interface: state -> bool (trade or do nothing). That’s enough to start generating data for “sell CC vs do nothing” as a first supervised/RL step.

# src/backtest/covered_call_simulator.py
from __future__ import annotations

from datetime import datetime, timedelta
from typing import Optional, List, Dict, Callable

import pandas as pd

from .deribit_data_source import DeribitDataSource, OptionSnapshot
from .types import CallSimulationConfig, SimulatedTrade, SimulationResult


State = dict                      # can later become a proper dataclass
PolicyFn = Callable[[State], bool]
# PolicyFn(state) -> True  => sell the call
# PolicyFn(state) -> False => do nothing


class CoveredCallSimulator:
    """
    Simple covered-call simulator.

    - simulate_single_call: 'what if I sold a 7DTE ~0.25Δ call at this time?'
    - simulate_policy: loop over many decision times and apply a simple policy
      that chooses SELL_CALL vs DO_NOTHING.
    """

    def __init__(self, data_source: DeribitDataSource, config: CallSimulationConfig):
        self.ds = data_source
        self.cfg = config

    # ---------- Helper: timeframe -> timedelta ----------

    @staticmethod
    def _timeframe_to_timedelta(tf: str) -> timedelta:
        if tf == "1m":
            return timedelta(minutes=1)
        if tf == "5m":
            return timedelta(minutes=5)
        if tf == "15m":
            return timedelta(minutes=15)
        if tf == "1h":
            return timedelta(hours=1)
        if tf == "4h":
            return timedelta(hours=4)
        if tf == "1d":
            return timedelta(days=1)
        raise ValueError(f"Unsupported timeframe: {tf}")

    # ---------- Helper: generate decision times ----------

    def _generate_decision_times(self) -> List[datetime]:
        """
        Generate decision timestamps between cfg.start and cfg.end
        using timeframe * decision_interval_bars.
        We also stop early if target_dte would push expiry beyond cfg.end.
        """
        tf_step = self._timeframe_to_timedelta(self.cfg.timeframe) * self.cfg.decision_interval_bars
        t = self.cfg.start
        decision_times: List[datetime] = []

        # Ensure we have enough room for target_dte before cfg.end
        max_t = self.cfg.end - timedelta(days=self.cfg.target_dte)

        while t <= max_t:
            decision_times.append(t)
            t += tf_step

        return decision_times

    # ---------- Helper: build simple state ----------

    def _build_state(self, as_of: datetime) -> State:
        """
        Extremely simple state representation for now:
        - time
        - underlying
        - spot price at as_of
        (you can enrich this later with IV, realized vol, etc.)
        """
        # Get a single spot candle around as_of by requesting a small window
        spot_df = self.ds.get_spot_ohlc(
            underlying=self.cfg.underlying,
            start=as_of - self._timeframe_to_timedelta(self.cfg.timeframe),
            end=as_of,
            timeframe=self.cfg.timeframe,
        )
        if spot_df.empty:
            spot_price = None
        else:
            spot_price = float(spot_df["close"].iloc[-1])

        return {
            "time": as_of,
            "underlying": self.cfg.underlying,
            "spot": spot_price,
        }

    # ---------- Option selection helper ----------

    def _find_target_call(self, as_of: datetime) -> Optional[OptionSnapshot]:
        """
        Find a call option for cfg.underlying with:
        - expiry ~ target_dte days from as_of (within tolerance),
        - delta ~ target_delta (within tolerance),
        and return the closest delta.
        """
        chain = self.ds.list_option_chain(self.cfg.underlying, as_of=as_of)
        if not chain:
            return None

        target_dte = self.cfg.target_dte
        dte_tol = self.cfg.dte_tolerance
        target_delta = self.cfg.target_delta
        delta_tol = self.cfg.delta_tolerance

        candidates: List[OptionSnapshot] = []
        for opt in chain:
            if opt.kind != "call":
                continue
            if opt.expiry <= as_of:
                continue

            dte_days = (opt.expiry - as_of).total_seconds() / 86400.0
            if abs(dte_days - target_dte) > dte_tol:
                continue

            if opt.delta is None:
                continue

            delta_val = float(opt.delta)
            if abs(delta_val - target_delta) > delta_tol:
                continue

            candidates.append(opt)

        if not candidates:
            return None

        best = min(candidates, key=lambda o: abs(float(o.delta) - target_delta))
        return best

    # ---------- Single trade simulation ----------

    def simulate_single_call(
        self,
        decision_time: datetime,
        size: float,
    ) -> Optional[SimulatedTrade]:
        """
        'If I sold size units of a 7DTE ~0.25Δ call at decision_time,
         what would PnL and drawdown have been vs HODL?'
        """

        target = self._find_target_call(as_of=decision_time)
        if target is None or target.mark_price is None:
            return None

        cfg = self.cfg
        ds = self.ds

        open_time = decision_time
        open_price = float(target.mark_price)

        # Spot path from decision -> expiry
        spot_df = ds.get_spot_ohlc(
            underlying=cfg.underlying,
            start=decision_time,
            end=target.expiry,
            timeframe=cfg.timeframe,
        )
        if spot_df.empty:
            return None

        # Option price path
        opt_df = ds.get_option_ohlc(
            instrument_name=target.instrument_name,
            start=decision_time,
            end=target.expiry,
            timeframe=cfg.timeframe,
        )
        if opt_df.empty:
            return None

        # Align timestamps
        idx = spot_df.index.union(opt_df.index).sort_values()
        spot = spot_df.reindex(idx).ffill()["close"]
        opt_price = opt_df.reindex(idx).ffill()["close"]

        portfolio_values: List[float] = []
        hodl_values: List[float] = []

        for ts in idx:
            s = float(spot.loc[ts])
            c = float(opt_price.loc[ts])

            hodl_val = size * s
            cc_val = size * s + size * (open_price - c)
            portfolio_values.append(cc_val)
            hodl_values.append(hodl_val)

        if not portfolio_values:
            return None

        start_portfolio = portfolio_values[0]
        start_hodl = hodl_values[0]
        final_cc = portfolio_values[-1]
        final_hodl = hodl_values[-1]
        final_opt_price = float(opt_price.iloc[-1])

        pnl = final_cc - start_portfolio
        pnl_vs_hodl = final_cc - final_hodl

        # Max drawdown on covered-call portfolio
        peak = portfolio_values[0]
        max_dd_pct = 0.0
        for v in portfolio_values:
            if v > peak:
                peak = v
            dd = (peak - v) / peak if peak > 0 else 0.0
            if dd > max_dd_pct:
                max_dd_pct = dd

        return SimulatedTrade(
            instrument_name=target.instrument_name,
            underlying=cfg.underlying,
            side="SHORT_CALL",
            size=size,
            open_time=open_time,
            close_time=target.expiry,
            open_price=open_price,
            close_price=final_opt_price,
            pnl=pnl,
            pnl_vs_hodl=pnl_vs_hodl,
            max_drawdown_pct=max_dd_pct * 100.0,
            notes=(
                f"target_dte={cfg.target_dte}, target_delta={cfg.target_delta}, "
                f"actual_delta={target.delta}, expiry={target.expiry.isoformat()}"
            ),
        )

    # ---------- Policy simulation across many decisions ----------

    def simulate_policy(
        self,
        policy: PolicyFn,
        size: Optional[float] = None,
    ) -> SimulationResult:
        """
        Loop over many decision times between cfg.start and cfg.end.
        For each decision time:
          - Build a simple state snapshot.
          - Ask policy(state) -> True/False (SELL_CALL vs DO_NOTHING).
          - If True, simulate a single 7DTE ~0.25Δ call trade from that time.
        For now we assume trades are independent and we aggregate PnL at trade close
        to build a rough equity curve.
        """

        trade_size = size if size is not None else self.cfg.initial_spot_position

        decision_times = self._generate_decision_times()
        trades: List[SimulatedTrade] = []
        equity_curve: Dict[datetime, float] = {}
        equity_vs_hodl: Dict[datetime, float] = {}

        # Start with baseline equity = 0 (we care about incremental effect)
        cumulative_pnl = 0.0
        cumulative_pnl_vs_hodl = 0.0

        for t in decision_times:
            state = self._build_state(as_of=t)

            # If we don't have spot data, skip
            if state["spot"] is None:
                continue

            take_trade = policy(state)
            if not take_trade:
                continue

            trade = self.simulate_single_call(decision_time=t, size=trade_size)
            if trade is None:
                continue

            trades.append(trade)

            # At trade close time, update cumulative pnl
            cumulative_pnl += trade.pnl
            cumulative_pnl_vs_hodl += trade.pnl_vs_hodl

            close_ts = trade.close_time
            equity_curve[close_ts] = cumulative_pnl
            equity_vs_hodl[close_ts] = cumulative_pnl_vs_hodl

        # Summary metrics
        if equity_curve:
            equity_series = pd.Series(equity_curve).sort_index()
            peak = equity_series.cummax()
            dd = (peak - equity_series) / peak.replace(0, float("nan"))
            max_dd = float(dd.max() * 100.0)
            final_equity = float(equity_series.iloc[-1])
        else:
            max_dd = 0.0
            final_equity = 0.0

        metrics = {
            "num_trades": len(trades),
            "final_pnl": final_equity,
            "max_drawdown_pct": max_dd,
        }

        return SimulationResult(
            trades=trades,
            equity_curve=equity_curve,
            equity_vs_hodl=equity_vs_hodl,
            metrics=metrics,
        )


This is not a full portfolio simulator (we’re assuming trades are “independent” and adding PnLs at close times), but it’s good enough for what we want next: state–action–reward data.

3. Wiring into an LLM training/scoring dataset

Now let’s add a tiny helper that:

Runs simulate_policy with a simple policy (e.g. “always trade” or some heuristic),

Emits rows of the form:

{
  "decision_time": ...,
  "underlying": "BTC",
  "spot": ...,
  "action": "SELL_CALL" or "DO_NOTHING",
  "reward": pnl_vs_hodl,   # or some function of pnl & drawdown
}


You can later:

Put this into CSV or Parquet,

Feed it into a training script (supervised model, RL, or as extra features for the LLM).

training_dataset.py (helper)
# src/backtest/training_dataset.py
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Any, Callable

from .covered_call_simulator import CoveredCallSimulator, State, PolicyFn
from .types import SimulationResult


@dataclass
class TrainingExample:
    decision_time: datetime
    underlying: str
    spot: float
    action: str             # "SELL_CALL" or "DO_NOTHING"
    reward: float           # e.g. pnl_vs_hodl
    extra: Dict[str, Any]   # place to stash any extra features


def always_trade_policy(state: State) -> bool:
    """
    Very simple baseline policy: always sell the call if we have a spot price.
    """
    return state.get("spot") is not None


def reward_from_trade(trade) -> float:
    """
    Define reward for a trade.
    For now, just use pnl_vs_hodl.
    You can later penalize drawdown here: pnl_vs_hodl - lambda * max_dd.
    """
    return float(trade.pnl_vs_hodl)


def generate_training_data(
    sim: CoveredCallSimulator,
    policy: PolicyFn | None = None,
) -> List[TrainingExample]:
    """
    Generate a simple dataset of (state, action, reward) triples.

    - We run simulate_policy with the given policy (default: always_trade_policy),
    - For each trade, we create a 'SELL_CALL' example with reward,
    - For decision times where we didn't trade, you can optionally add 'DO_NOTHING' examples later.
    """
    policy_fn = policy or always_trade_policy

    # Run the policy to get trades and equity stats
    result: SimulationResult = sim.simulate_policy(policy_fn)

    examples: List[TrainingExample] = []
    for trade in result.trades:
        examples.append(
            TrainingExample(
                decision_time=trade.open_time,
                underlying=trade.underlying,
                spot=None,  # we could re-lookup spot here if needed
                action="SELL_CALL",
                reward=reward_from_trade(trade),
                extra={
                    "instrument_name": trade.instrument_name,
                    "size": trade.size,
                    "max_drawdown_pct": trade.max_drawdown_pct,
                },
            )
        )

    return examples


This is deliberately simple:

First version only creates “SELL_CALL” examples (no DO_NOTHING); that’s still useful if later you:

Compare SELL_CALL vs implied baseline of “don’t sell”.

Later you can:

Generate a second run where policy = “never trade” and label those as DO_NOTHING, reward = 0, etc.

Or explicitly compute DO_NOTHING reward for each decision time.

You can then dump examples to CSV/Parquet and use it however you want.

4. What to tell your AI builder

You can tell Replit something like:

Add the backtest modules (data_source.py, deribit_client.py, deribit_data_source.py, types.py, covered_call_simulator.py, training_dataset.py) under src/backtest/.

Make sure imports are correct (from src.backtest... if your package root is src).

Do not tie this into the FastAPI/worker startup – it should only be used from offline scripts or notebooks.

Optionally add a backtest_example.py entry point to run a single BTC test and print the simulated trade.

Then later, once you have some TrainingExample CSVs, we can talk about how to:

Use them as features for the LLM (e.g. “this candidate historically has reward ~0.7”),

Or train a small numeric scoring model that the LLM can consult.

For now, this gives you:

A way to replay “what if I sold this 7DTE 0.25Δ call” over many dates,

PnL + drawdown + incremental vs HODL,

And a pipeline to turn it into state–action–reward rows for training.