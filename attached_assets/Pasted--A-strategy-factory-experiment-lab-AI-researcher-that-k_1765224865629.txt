“A strategy factory + experiment lab + AI researcher that keeps tuning bots.”

That’s ambitious, but it fits really well with the architecture you already have (backtester, PostgreSQL runs, LLM layer, etc.).

I’ll break it into:

Is this a good idea? What’s great / what to watch out for

How I’d design it in your system (concrete pieces)

How far we can realistically push “fully autonomous” without it going off the rails

1. Is this a good idea?
The good parts

All of this is strong:

Creating bots/strategies from the UI
→ That’s exactly what your new Strategy abstraction + shared state builder is meant for. It moves you from “one hard-coded bot” to “configurable bots with parameters.”

Being able to run the same bot config:

live on testnet,

on synthetic historical data,

on harvested real data
→ You already have SYNTHETIC, LIVE_DERIBIT, REAL_SCRAPER data sources. This is a perfect fit.

LLM as analyst and advisor:

“Here are your last 20 runs, here’s why they’re good/bad, here’s what to change.”
→ This is exactly where LLMs shine: interpreting metrics, seeing patterns, explaining trade-offs.

Iterative loop:

run backtest → AI suggests new params → run again → compare → converge
→ This is basically AutoML for trading strategies.

The danger zone

Where it can go wrong:

Overfitting hell
If the bot keeps optimizing on the same dataset, it will find “magic parameters” that only work on that specific period and die in real life.
→ We need train vs validation time periods and clear stopping rules.

LLM as random parameter generator
Letting LLM arbitrarily pick deltas/DTEs each time is less reliable than using numeric search (grid/random/Bayesian).
→ LLM should guide and interpret, not brute-force search the param space alone.

Complexity explosion
Full “autonomous experiment runner” can get complicated quickly (queues, limits, monitoring).
→ We should design it in layers: manual → semi-auto → mostly-auto.

So: the idea is good, but it needs structure so it doesn’t turn into a clever curve-fitter that just discovers the past.

2. How to design this in your system

Let’s design the pieces using names that match your world (backtester, Postgres, etc.).

2.1 Core data models

You basically need three main “things”:

1) StrategyDefinition / Bot Definition

Think of this as “what this bot is allowed to do” and “what it’s trying to optimize”.

Example fields:

id, name, description

base_strategy_type: "covered_call" | "wheel" | "other"

risk_profile: "conservative" | "balanced" | "aggressive" | "custom"

Parameters:

delta_range: [0.15, 0.35]

dte_range: [7, 21]

ivrv_min: e.g. 1.2

exit_style: "tp_and_roll" | "hold_to_expiry"

position_size_rule: e.g. "1pct_equity", "fixed_0.1_btc"

LLM settings:

policy_mode: "rule_only" | "llm_only | "hybrid"

hybrid_llm_prob: e.g. 0.25

llm_allowed_to_edit_params: false (for now)

Lifecycle:

status: "draft" | "backtesting" | "testnet_live" | "mainnet_live | "paused"

This can live in your Postgres DB and be editable from the UI.

2) BacktestRun / Experiment

This you almost already have in Postgres; we just formalize it as “experiments for a given strategy definition”.

Fields:

id, strategy_id (FK -> StrategyDefinition)

data_source: "synthetic" | "live_deribit" | "real_scraper"

time_range: [start_date, end_date]

decision_interval: "1h" | "4h" | "1d" | ...

policy_mode & llm_prob used in that run

Metrics:

net PnL, PnL %, max drawdown, Sharpe/Sortino, win rate, num_trades, etc.

Run metadata:

timestamp, code version, config snapshot

This is what your compare_synthetic_vs_live.py already writes; we just treat it as first-class objects.

3) LLMAnalysis / Research note

This is the AI’s “research report” attached to one or more runs.

Fields:

id, strategy_id

runs_considered: [run_id, run_id, ...]

summary_text: human-readable explanation

suggested_changes: structured JSON, e.g.:

{
  "delta_range": [0.20, 0.30],
  "dte_range": [7, 14],
  "ivrv_min": 1.3,
  "position_size_rule": "0_7pct_equity",
  "notes": [
    "2022 bear regime shows too much drawdown; tighten delta",
    "IVRV often under 1.1 in quiet regimes; skip those"
  ]
}


Whether you accepted those changes and created a new StrategyDefinition version.

2.2 UI flow
A) “Create New Bot / Strategy” wizard

From the UI:

+ New Strategy

Step 1 – Template:

Choose base: Covered Call / later Wheel.

Step 2 – Profile:

Conservative / Balanced / Aggressive / Custom.

Step 3 – Parameters:

Delta range slider: e.g. 0.15–0.30

DTE range slider: e.g. 7–21

IVRV min

Exit style (TP+roll vs hold)

Step 4 – LLM settings:

Policy mode: Rule / LLM / Hybrid

LLM probability (if hybrid)

Save → This creates a StrategyDefinition row.

Then you have buttons like:

Run Backtest → pick data source & time range

Start on Testnet → run live loop with that StrategyDefinition

(Later) Promote to Mainnet

B) “Backtest & Optimize” tab for each strategy

On a strategy’s details page:

Table of past runs:

Run	Source	Period	Policy	PnL %	Max DD	Sharpe	Notes
#17	synthetic	2021+2022	hybrid	24%	18%	1.1	baseline
#18	live_deribit	2023 Q1–Q4	rule	9%	12%	0.7	lower vol
...	...	...	...	...	...	...	...

Controls:

Run new backtest (choose data source, time period)

Ask AI for suggestions (one-shot)

Auto-tune (N steps) (semi-automatic loop, see below)

2.3 The optimization loop: 3 levels
Level 1 – Manual + AI Advisor (short-term, easy)

You run 3–10 backtests for a strategy.

On the UI, click “Ask AI for suggestions”.

Backend collects:

a table of the last N runs (metrics),

maybe some regime descriptors (bull/bear, vol).

Sends that as context to the LLM, with a prompt like:

“Given these runs and constraints, propose a SMALL change to parameters that’s likely to improve Sharpe without increasing max drawdown beyond X%.”

LLM returns suggested_changes JSON + explanation.

You see:

“Increase IVRV min to 1.3, tighten delta range, shorten DTE in volatile periods” etc.

You click “Apply as new version” or ignore.

This is low-risk: you’re in the loop.

Level 2 – Semi-auto “Auto-Tune up to N runs”

Here we give it limited autonomy.

Workflow:

On a strategy page:
“Auto-tune this strategy for up to 20 backtests. Goal: max Sharpe, max DD ≤ 30%.”

Behind the scenes:

We define a parameter space:

delta_min ∈ [0.10, 0.30]

delta_max ∈ [0.20, 0.50]

dte_min ∈ [3, 10]

dte_max ∈ [10, 45]

ivrv_min ∈ [1.0, 1.5]

Use a numeric search algorithm (random search or Bayesian optimization) to pick the next parameter set.
→ This part is pure math, deterministic, reliable.

Each time a run finishes:

metrics → DB

we ask LLM for commentary:

“How are we doing? Are we just fitting 2021 bull, or robust across 2022 bear?”

“Which region of parameter space looks promising?”

The loop stops when:

we hit N runs, or

no improvement after X iterations, or

user hits “Stop”.

At the end, we:

mark the best config as “recommended config” for that StrategyDefinition.

attach an LLMAnalysis note summarizing why.

This gets you closer to your vision: LLM + numeric optimizer co-design the bot, but with:

bounded param ranges,

a max number of experiments,

a clear audit trail.

Level 3 – Mostly-auto (but still bounded)

Full autonomy looks like:

When you create a new strategy, system automatically:

runs an initial grid/random sweep,

has LLM refine direction a few times,

stops when metrics plateau,

presents you: “Here is the best config found; here is the performance across train vs validation periods; do you want to start this on testnet?”

We can absolutely get there, but I’d gate it with:

Time split:

e.g. 2019–2022 “training”, 2023–2024 “validation”.

Hard limits:

max experiments per strategy,

no auto-promotion to live without your manual click.

3. How “autonomous” can it safely be?

You said:

“Ideally, it would run completely automatically and autonomously… run again until it finds the best setup.”

Technically: yes, we can build a loop where:

An orchestrator process watches the DB for strategy definitions that are in “tuning” mode.

It triggers backtests with different params.

LLM interprets results and requests new parameter suggestions.

It repeats until a stopping criterion is met.

Practically, I’d stage it:

Now (Phase 2)

Implement:

Strategy definitions in DB,

UI bot creation wizard,

Backtest runs tied to strategies,

“Ask AI for suggestions” (Level 1),

Optional “Auto-tune up to N runs” using numeric search (Level 2).

Keep you firmly in the loop.

Later (Phase 3)

Hook this up to multi-bot + Supervisor LLM:

Supervisor decides which strategies deserve more tuning.

Maybe schedules tuning overnight.

Allow “mostly auto” loops only inside a training sandbox.

We absolutely don’t want a runaway situation where LLM is constantly tweaking live strategies with no human review.

TL;DR

Your idea is solid and aligns perfectly with the architecture you already have:
Strategy definitions + multiple data sources + backtester + Postgres + LLM = strategy lab with AI researcher.

Best design:

Store strategies as first-class objects (with parameters, goals, LLM settings).

Attach backtest runs and AI analyses to them.

Let the LLM act as:

Analyst (interpret results),

Heuristic guide (where to search next),

and let numeric optimization handle the heavy lifting of parameter search.

Autonomy is doable, but I’d roll it out in layers:

Manual + AI suggestions → Semi-auto with limits → Mostly-auto in a sandbox.